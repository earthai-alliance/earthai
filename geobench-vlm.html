<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="GEOBench-VLM - Benchmarking Vision-Language Models for Geospatial Tasks">
  <meta name="keywords" content="GEOBench, Vision-Language Models, Benchmark, Geospatial AI, Remote Sensing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GEOBench-VLM - Earth Vision AI Alliance</title>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    html {
      scroll-behavior: smooth;
    }

    body {
      font-family: 'Inter', sans-serif;
      line-height: 1.6;
      color: #333;
      overflow-x: hidden;
    }

    /* Navbar */
    .navbar {
      position: fixed;
      top: 0;
      width: 100%;
      background: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(10px);
      box-shadow: 0 2px 20px rgba(0,0,0,0.1);
      z-index: 1000;
      transition: all 0.3s ease;
    }

    .nav-container {
      max-width: 1200px;
      margin: 0 auto;
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 1rem 2rem;
    }

    .nav-logo {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      text-decoration: none;
      color: #333;
      font-weight: 700;
    }

    .nav-logo-img {
      height: 60px;
      width: auto;
    }

    .nav-logo-text {
      display: flex;
      flex-direction: column;
      line-height: 1.2;
    }

    .nav-logo-main {
      font-size: 1rem;
      font-weight: 700;
    }

    .nav-logo-sub {
      font-size: 0.75rem;
      color: #667eea;
      font-weight: 600;
    }

    .back-button {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      text-decoration: none;
      border-radius: 8px;
      font-weight: 600;
      transition: all 0.3s ease;
    }

    .back-button:hover {
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
    }

    /* Hero Section */
    .hero {
      min-height: 60vh;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 120px 2rem 4rem;
      position: relative;
      overflow: hidden;
    }

    .hero::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml,<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg"><circle cx="50" cy="50" r="2" fill="rgba(255,255,255,0.1)"/></svg>');
      opacity: 0.5;
    }

    .hero-content {
      max-width: 1200px;
      text-align: center;
      color: white;
      z-index: 1;
    }

    .hero-content h1 {
      font-size: 4rem;
      font-weight: 700;
      margin-bottom: 1rem;
    }

    .hero-content p {
      font-size: 1.3rem;
      opacity: 0.95;
      max-width: 800px;
      margin: 0 auto;
    }

    /* Section Styles */
    section {
      padding: 5rem 2rem;
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
    }

    .section-title {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 3rem;
      text-align: center;
      position: relative;
      display: block;
      width: 100%;
    }

    .section-title::after {
      content: '';
      position: absolute;
      bottom: -10px;
      left: 50%;
      transform: translateX(-50%);
      width: 60px;
      height: 4px;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border-radius: 2px;
    }

    /* Overview Section */
    .overview {
      background: white;
    }

    .overview-text {
      max-width: 900px;
      margin: 0 auto;
      font-size: 1.1rem;
      line-height: 1.8;
      color: #555;
    }

    /* Projects Grid */
    #subprojects {
      background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    }

    .projects-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 2.5rem;
      max-width: 1200px;
      margin: 0 auto;
    }

    .project-card {
      background: white;
      border-radius: 15px;
      overflow: hidden;
      box-shadow: 0 4px 15px rgba(0,0,0,0.08);
      transition: all 0.3s ease;
      display: flex;
      flex-direction: column;
    }

    .project-card:hover {
      transform: translateY(-10px);
      box-shadow: 0 10px 30px rgba(0,0,0,0.15);
    }

    .project-image {
      width: 100%;
      height: 250px;
      object-fit: cover;
    }

    .project-content {
      padding: 2rem;
      flex: 1;
      display: flex;
      flex-direction: column;
    }

    .project-title {
      font-size: 1.6rem;
      font-weight: 700;
      margin-bottom: 1rem;
      color: #333;
    }

    .project-links {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      margin-bottom: 1.5rem;
    }

    .project-link {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.5rem 1rem;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      text-decoration: none;
      border-radius: 6px;
      font-size: 0.9rem;
      font-weight: 600;
      transition: all 0.3s ease;
    }

    .project-link:hover {
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
    }

    .project-description {
      color: #666;
      line-height: 1.8;
      margin-bottom: 1.5rem;
      flex: 1;
    }

    .project-paper {
      padding: 1rem;
      background: #f8f9fa;
      border-left: 4px solid #667eea;
      border-radius: 4px;
      font-size: 0.9rem;
      color: #555;
      line-height: 1.6;
      margin-top: auto;
    }

    .project-paper strong {
      color: #667eea;
    }

    /* Responsive */
    @media (max-width: 768px) {
      .hero-content h1 {
        font-size: 2.5rem;
      }

      .projects-grid {
        grid-template-columns: 1fr;
      }

      .nav-logo-img {
        height: 50px;
      }

      .project-links {
        flex-direction: column;
      }
    }
  </style>
</head>
<body>

<!-- Navbar -->
<nav class="navbar">
  <div class="nav-container">
    <a href="index.html" class="nav-logo">
      <svg class="nav-logo-img" viewBox="0 0 400 400" xmlns="http://www.w3.org/2000/svg">
        <defs>
          <linearGradient id="earthGradient" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#667eea;stop-opacity:1" />
            <stop offset="100%" style="stop-color:#764ba2;stop-opacity:1" />
          </linearGradient>
          <linearGradient id="orbitGradient" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#00d4ff;stop-opacity:1" />
            <stop offset="100%" style="stop-color:#667eea;stop-opacity:1" />
          </linearGradient>
        </defs>
        <circle cx="200" cy="200" r="80" fill="url(#earthGradient)"/>
        <g fill="rgba(255,255,255,0.3)">
          <path d="M 160 160 Q 150 170 155 180 Q 160 190 170 185 Q 180 180 175 170 Q 170 160 160 160 Z"/>
          <path d="M 210 170 Q 200 175 205 185 Q 210 195 220 190 Q 230 185 225 175 Q 220 165 210 170 Z"/>
          <path d="M 230 155 Q 220 160 225 170 Q 230 180 240 175 Q 250 170 245 160 Q 240 150 230 155 Z"/>
          <path d="M 180 210 Q 175 220 180 230 Q 185 240 190 235 Q 195 230 190 220 Q 185 210 180 210 Z"/>
        </g>
        <g fill="white" opacity="0.8">
          <circle cx="170" cy="190" r="3"/>
          <circle cx="190" cy="180" r="3"/>
          <circle cx="210" cy="195" r="3"/>
          <circle cx="195" cy="210" r="3"/>
          <circle cx="215" cy="175" r="3"/>
          <circle cx="230" cy="190" r="3"/>
        </g>
        <g stroke="white" stroke-width="1" opacity="0.4" fill="none">
          <line x1="170" y1="190" x2="190" y2="180"/>
          <line x1="190" y1="180" x2="210" y2="195"/>
          <line x1="210" y1="195" x2="195" y2="210"/>
          <line x1="190" y1="180" x2="215" y2="175"/>
          <line x1="215" y1="175" x2="230" y2="190"/>
          <line x1="210" y1="195" x2="230" y2="190"/>
        </g>
        <g fill="none" stroke="url(#orbitGradient)" stroke-width="2" opacity="0.6">
          <ellipse cx="200" cy="200" rx="110" ry="50" transform="rotate(20 200 200)"/>
          <ellipse cx="200" cy="200" rx="110" ry="50" transform="rotate(-20 200 200)"/>
          <ellipse cx="200" cy="200" rx="110" ry="50" transform="rotate(60 200 200)"/>
        </g>
        <g fill="#00d4ff">
          <circle cx="310" cy="200" r="4"/>
          <circle cx="90" cy="200" r="4"/>
          <circle cx="200" cy="120" r="4"/>
          <circle cx="200" cy="280" r="4"/>
        </g>
        <g transform="translate(200, 200)">
          <path d="M -25 0 Q -25 -15 0 -20 Q 25 -15 25 0 Q 25 15 0 20 Q -25 15 -25 0 Z" 
                fill="none" stroke="white" stroke-width="2" opacity="0.7"/>
          <circle cx="0" cy="0" r="8" fill="white" opacity="0.9"/>
          <circle cx="0" cy="0" r="5" fill="#667eea"/>
          <circle cx="-2" cy="-2" r="2" fill="white"/>
        </g>
      </svg>
      <div class="nav-logo-text">
        <span class="nav-logo-main">EARTH VISION</span>
        <span class="nav-logo-sub">AI Alliance</span>
      </div>
    </a>
    <a href="index.html#projects" class="back-button">
      <i class="fas fa-arrow-left"></i>
      Back to Home
    </a>
  </div>
</nav>

<!-- Hero -->
<section class="hero">
  <div class="hero-content">
    <h1>GEOBench-VLM</h1>
    <p>Benchmarking Vision-Language Models for Geospatial Tasks</p>
  </div>
</section>

<!-- Overview -->
<section class="overview">
  <div class="container">
    <h2 class="section-title">Overview</h2>
    <div class="overview-text">
      <p>
        GEOBench-VLM is a comprehensive benchmarking framework designed to evaluate vision-language models on the unique challenges of geospatial data. Unlike traditional computer vision benchmarks, GEOBench-VLM addresses domain-specific requirements including temporal analysis, fine-grained object detection in satellite imagery, damage assessment, and complex spatial reasoning tasks.
      </p>
      <p style="margin-top: 1.5rem;">
        The benchmark provides a standardized evaluation methodology for assessing model capabilities across diverse Earth observation scenarios, helping researchers and practitioners understand model strengths, limitations, and guide future development of geospatial AI systems. GEOBench-VLM has evolved through multiple iterations, incorporating community feedback and expanding task coverage.
      </p>
    </div>
  </div>
</section>

<!-- Related Projects -->
<section id="subprojects">
  <div class="container">
    <h2 class="section-title">Related Research</h2>
    <div class="projects-grid">
      
      <!-- Geo-bench-2 -->
      <div class="project-card">
        <img src="static/images/geobench2.png" alt="Geo-bench-2" class="project-image">
        <div class="project-content">
          <h3 class="project-title">Geo-bench-2</h3>
          <div class="project-links">
            <a href="https://arxiv.org/pdf/2511.15658" target="_blank" class="project-link">
              <i class="fas fa-file-pdf"></i> arXiv
            </a>
          </div>
          <p class="project-description">
            An evolution of the benchmarking framework that shifts focus from pure performance metrics to capability assessment. Geo-bench-2 provides deeper insights into what geospatial AI models can actually do, moving beyond simple accuracy scores to understand functional capabilities and limitations across diverse Earth observation tasks.
          </p>
          <div class="project-paper">
            <strong>[arXiv 2025]</strong> Simumba, Naomi, et al. "Geo-bench-2: From performance to capability, rethinking evaluation in geospatial ai." arXiv preprint arXiv:2511.15658 (2025).
          </div>
        </div>
      </div>

      <!-- GEOBench-VLM (Original) -->
      <div class="project-card">
        <img src="static/images/geo_bench_vlm.png" alt="GEOBench-VLM" class="project-image">
        <div class="project-content">
          <h3 class="project-title">GEOBench-VLM</h3>
          <div class="project-links">
            <a href="https://the-ai-alliance.github.io/GEO-Bench-VLM/" target="_blank" class="project-link">
              <i class="fas fa-globe"></i> Project Page
            </a>
            <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.html" target="_blank" class="project-link">
              <i class="fas fa-file-pdf"></i> ICCV Paper
            </a>
          </div>
          <p class="project-description">
            The foundational benchmark for evaluating vision-language models on geospatial tasks. GEOBench-VLM introduces comprehensive evaluation protocols for temporal analysis, object detection, damage assessment, and spatial reasoning, establishing standards for measuring VLM performance in Earth observation applications.
          </p>
          <div class="project-paper">
            <strong>[ICCV 2025]</strong> Danish, Muhammad, et al. "Geobench-vlm: Benchmarking vision-language models for geospatial tasks." Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2025.
          </div>
        </div>
      </div>

      <!-- EarthDial -->
      <div class="project-card">
        <img src="static/images/EarthDial_Model_Archiecture.png" alt="EarthDial" class="project-image">
        <div class="project-content">
          <h3 class="project-title">EarthDial</h3>
          <div class="project-links">
            <a href="https://github.com/hiyamdebary/EarthDial" target="_blank" class="project-link">
              <i class="fab fa-github"></i> GitHub
            </a>
            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Soni_EarthDial_Turning_Multi-sensory_Earth_Observations_to_Interactive_Dialogues_CVPR_2025_paper.html" target="_blank" class="project-link">
              <i class="fas fa-file-pdf"></i> CVPR Paper
            </a>
          </div>
          <p class="project-description">
            A practical application demonstrating the capabilities measured by GEOBench-VLM. EarthDial transforms multi-sensory Earth observations into interactive dialogues, showcasing how vision-language models can enable natural language interfaces for complex geospatial analysis and decision-making tasks.
          </p>
          <div class="project-paper">
            <strong>[CVPR 2025]</strong> Soni, Sagar, et al. "Earthdial: Turning multi-sensory earth observations to interactive dialogues." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2025.
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

</body>
</html>

